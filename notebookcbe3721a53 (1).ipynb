{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom torchvision.io import read_image\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom cv2 import putText, rectangle\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nseed = 2022\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-30T01:28:52.140317Z","iopub.execute_input":"2022-01-30T01:28:52.140952Z","iopub.status.idle":"2022-01-30T01:28:54.074127Z","shell.execute_reply.started":"2022-01-30T01:28:52.140911Z","shell.execute_reply":"2022-01-30T01:28:54.073389Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"[here](https://github.com/usef-kh/fer).  [here]('https://arxiv.org/abs/2105.03588'), written by authors Yousif Khaireddin and Zhuofa Chen.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/usef-kh/fer\n\nimport fer\n\nfrom fer.models import vgg","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:28:54.075751Z","iopub.execute_input":"2022-01-30T01:28:54.076008Z","iopub.status.idle":"2022-01-30T01:28:55.943268Z","shell.execute_reply.started":"2022-01-30T01:28:54.075974Z","shell.execute_reply":"2022-01-30T01:28:55.942323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"![network_architecture](https://github.com/usef-kh/fer/raw/master/images/architecture.jpeg)","metadata":{}},{"cell_type":"code","source":"emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n\nemotions_mapping = dict([(emotions[i], i) for i in range(len(emotions))])\n\nprint(emotions_mapping)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:28:55.946682Z","iopub.execute_input":"2022-01-30T01:28:55.946923Z","iopub.status.idle":"2022-01-30T01:28:55.952929Z","shell.execute_reply.started":"2022-01-30T01:28:55.946893Z","shell.execute_reply":"2022-01-30T01:28:55.951550Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Function to load a dataframe given a filepath (string).  Works for both training and test sets.\ndef create_dataframe(dataset_filepath):\n    images = []\n    labels = []\n    \n    # The images are stored in sub-folders corresponding to their labelled expression, we need to loop\n    # through these folders.\n    for i in range(len(emotions)):\n        paths = []\n        for _, _, path in os.walk(dataset_filepath + emotions[i]):\n            paths.extend(path)\n        label = [emotions[i]] * len(paths)\n        images.extend(paths), labels.extend(label)\n        \n    # Create a dataframe with two columns; the first with the path to the image, \n    # the second with the label for the expression.\n    df = pd.DataFrame(list(zip(images, labels)), columns=['image_path', 'emotion'])\n    df['image_path'] = df['emotion'] + '/' + df['image_path']\n    df['image_path'] = df['image_path'].apply(lambda x: dataset_filepath + x)\n    df['encoded_emotion'] = df['emotion'].map(emotions_mapping)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:28:55.955122Z","iopub.execute_input":"2022-01-30T01:28:55.955993Z","iopub.status.idle":"2022-01-30T01:28:55.968675Z","shell.execute_reply.started":"2022-01-30T01:28:55.955951Z","shell.execute_reply":"2022-01-30T01:28:55.967965Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/fer2013/train/'\n\n# Call the function to create the dataframe\ntrain_df = create_dataframe(train_path)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:28:55.970027Z","iopub.execute_input":"2022-01-30T01:28:55.970305Z","iopub.status.idle":"2022-01-30T01:29:13.659046Z","shell.execute_reply.started":"2022-01-30T01:28:55.970267Z","shell.execute_reply":"2022-01-30T01:29:13.658338Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Print the number of faces for each expression\ntrain_df['emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:29:16.619087Z","iopub.execute_input":"2022-01-30T01:29:16.619353Z","iopub.status.idle":"2022-01-30T01:29:16.632640Z","shell.execute_reply.started":"2022-01-30T01:29:16.619316Z","shell.execute_reply":"2022-01-30T01:29:16.631708Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Plot 5 randomly samples faces from each class.\nplt.figure(figsize=(12, 12))\nidx = train_df.groupby('emotion').sample(n=5, random_state=seed).index\nfor i, index in enumerate(idx):\n    plt.subplot(7, 5, i+1)\n    plt.imshow(np.squeeze(read_image(train_df['image_path'][index])), cmap='gray')\n    plt.axis('off')\n    plt.title(f'{train_df[\"emotion\"][index]}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:29:19.977178Z","iopub.execute_input":"2022-01-30T01:29:19.978017Z","iopub.status.idle":"2022-01-30T01:29:21.684682Z","shell.execute_reply.started":"2022-01-30T01:29:19.977970Z","shell.execute_reply":"2022-01-30T01:29:21.684000Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Look at some more neutral expressions\nplt.figure(figsize=(12, 12))\nidx = train_df.loc[train_df['emotion'] == 'neutral'].sample(n=25, random_state=seed).index\nfor i, index in enumerate(idx):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(np.squeeze(read_image(train_df['image_path'][index])), cmap='gray')\n    plt.axis('off')\n    plt.title(f'{train_df[\"emotion\"][index]}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:29:21.686022Z","iopub.execute_input":"2022-01-30T01:29:21.686243Z","iopub.status.idle":"2022-01-30T01:29:23.063024Z","shell.execute_reply.started":"2022-01-30T01:29:21.686215Z","shell.execute_reply":"2022-01-30T01:29:23.059820Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Declare the model and load the pretrained weights\nmodel = vgg.Vgg()\n\nfpath = '../input/facial-emotion-recognition-vggnet-state/VGGNet'\n\ncheckpoint = torch.load(fpath, map_location=device)\n\nmodel.load_state_dict(checkpoint['params'])\n\nmodel = model.to(device)#.eval()\n\nmodel.drop = model.drop.eval()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:30:05.217625Z","iopub.execute_input":"2022-01-30T01:30:05.218408Z","iopub.status.idle":"2022-01-30T01:30:11.224278Z","shell.execute_reply.started":"2022-01-30T01:30:05.218370Z","shell.execute_reply":"2022-01-30T01:30:11.223465Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.TenCrop(40),\n    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n    transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n])\nmu, st = 0, 255","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:30:17.191419Z","iopub.execute_input":"2022-01-30T01:30:17.192189Z","iopub.status.idle":"2022-01-30T01:30:17.198601Z","shell.execute_reply.started":"2022-01-30T01:30:17.192136Z","shell.execute_reply":"2022-01-30T01:30:17.197613Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load the first image in the dataset and plot the crops as a visualisation of the models input\nimg = Image.open(train_df['image_path'][0])\nimg = test_transform(img)\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.title(f'{train_df[\"emotion\"][0]}')\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.squeeze(img[i, :, :, :].numpy()), cmap='gray')\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:30:20.219455Z","iopub.execute_input":"2022-01-30T01:30:20.220155Z","iopub.status.idle":"2022-01-30T01:30:20.708905Z","shell.execute_reply.started":"2022-01-30T01:30:20.220109Z","shell.execute_reply":"2022-01-30T01:30:20.708232Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create a dataset class to load batched images\nclass FER2013Dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        images = self.images[idx]\n        images = Image.open(images)\n        \n        if self.transform:\n            images = self.transform(images)\n            \n        labels = torch.tensor(self.labels[idx]).type(torch.long)\n        \n        return images, labels","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:30:22.633343Z","iopub.execute_input":"2022-01-30T01:30:22.633622Z","iopub.status.idle":"2022-01-30T01:30:22.641708Z","shell.execute_reply.started":"2022-01-30T01:30:22.633591Z","shell.execute_reply":"2022-01-30T01:30:22.640408Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load the test set\ntest_path = '../input/fer2013/test/'\ntest_df = create_dataframe(test_path)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:30:25.791667Z","iopub.execute_input":"2022-01-30T01:30:25.792150Z","iopub.status.idle":"2022-01-30T01:30:31.366107Z","shell.execute_reply.started":"2022-01-30T01:30:25.792110Z","shell.execute_reply":"2022-01-30T01:30:31.365390Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Create the dataloader to pass batches to the model\ntest_dataset = FER2013Dataset(test_df['image_path'], test_df['encoded_emotion'], test_transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:30:31.367499Z","iopub.execute_input":"2022-01-30T01:30:31.368262Z","iopub.status.idle":"2022-01-30T01:30:31.373569Z","shell.execute_reply.started":"2022-01-30T01:30:31.368221Z","shell.execute_reply":"2022-01-30T01:30:31.372874Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    full_loss, n_correct = 0, 0\n    n_samples = 0\n\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(dataloader)):\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            n_samples += labels.shape[0]\n            # fuse crops and batchsize\n            bs, ncrops, c, h, w = images.shape\n            images = images.view(-1, c, h, w)\n\n            # forward\n            outputs = model(images)\n\n            # combine results across the crops\n            outputs = outputs.view(bs, ncrops, -1)\n            outputs = (torch.sum(outputs, dim=1) / ncrops)\n            \n            loss = criterion(outputs, labels)\n            \n            outputs = outputs.argmax(axis=1)            \n            n_correct += (outputs == labels).sum()\n\n            full_loss += loss.item()\n\n\n    acc = n_correct / len(test_dataloader.dataset)\n    loss = full_loss / len(test_dataloader.dataset)\n\n    print(n_correct)\n    print(acc, loss, n_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:35:56.205834Z","iopub.execute_input":"2022-01-30T01:35:56.206390Z","iopub.status.idle":"2022-01-30T01:35:56.216148Z","shell.execute_reply.started":"2022-01-30T01:35:56.206347Z","shell.execute_reply":"2022-01-30T01:35:56.214917Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"evaluate(model, test_dataloader, torch.nn.CrossEntropyLoss())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:35:56.724282Z","iopub.execute_input":"2022-01-30T01:35:56.724660Z","iopub.status.idle":"2022-01-30T01:36:19.160414Z","shell.execute_reply.started":"2022-01-30T01:35:56.724624Z","shell.execute_reply":"2022-01-30T01:36:19.159298Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!pip install pytube\n\nfrom pytube import YouTube","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:36:35.934491Z","iopub.execute_input":"2022-01-30T01:36:35.935291Z","iopub.status.idle":"2022-01-30T01:36:45.263504Z","shell.execute_reply.started":"2022-01-30T01:36:35.935245Z","shell.execute_reply":"2022-01-30T01:36:45.262690Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"yt = YouTube('https://www.youtube.com/watch?v=luLpdr4n8m4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:36:45.265528Z","iopub.execute_input":"2022-01-30T01:36:45.265836Z","iopub.status.idle":"2022-01-30T01:36:45.270467Z","shell.execute_reply.started":"2022-01-30T01:36:45.265796Z","shell.execute_reply":"2022-01-30T01:36:45.269748Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Markdown as md\n\n# Show the title and thumbnail of the video url\nprint(yt.title)\nmd(f\"![]({yt.thumbnail_url})\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:36:48.332733Z","iopub.execute_input":"2022-01-30T01:36:48.333038Z","iopub.status.idle":"2022-01-30T01:36:48.529765Z","shell.execute_reply.started":"2022-01-30T01:36:48.333005Z","shell.execute_reply":"2022-01-30T01:36:48.529014Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Print all the streams available for download\nfor x in yt.streams:\n    print(x)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:36:49.855419Z","iopub.execute_input":"2022-01-30T01:36:49.856000Z","iopub.status.idle":"2022-01-30T01:36:51.349243Z","shell.execute_reply.started":"2022-01-30T01:36:49.855958Z","shell.execute_reply":"2022-01-30T01:36:51.348547Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Declare a filepath and download the stream \nvideo_clips_path = '/kaggle/working/video_clips/'\n\nstream = yt.streams.filter(file_extension='mp4', progressive=True).get_highest_resolution()\n\nstream.download(video_clips_path, 'test.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:36:54.144361Z","iopub.execute_input":"2022-01-30T01:36:54.145140Z","iopub.status.idle":"2022-01-30T01:36:54.465788Z","shell.execute_reply.started":"2022-01-30T01:36:54.145096Z","shell.execute_reply":"2022-01-30T01:36:54.464941Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"!pip install moviepy\nfrom moviepy.editor import *\n\nclip = VideoFileClip('./video_clips/test.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:36:58.213358Z","iopub.execute_input":"2022-01-30T01:36:58.213649Z","iopub.status.idle":"2022-01-30T01:37:42.233045Z","shell.execute_reply.started":"2022-01-30T01:36:58.213618Z","shell.execute_reply":"2022-01-30T01:37:42.231992Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Video\n\nVideo('./video_clips/test.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:37:42.235067Z","iopub.execute_input":"2022-01-30T01:37:42.235313Z","iopub.status.idle":"2022-01-30T01:37:42.241208Z","shell.execute_reply.started":"2022-01-30T01:37:42.235284Z","shell.execute_reply":"2022-01-30T01:37:42.240298Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def flip_frame(gf, t):\n    frame = gf(t)\n    frame = cv2.flip(frame, 0)\n    return frame\n\nfrom moviepy.tools import find_extension\nfrom moviepy.video.io.ffmpeg_writer import ffmpeg_write_video\n\ndef create_videofile(clip, filename):\n    audio_fps = 44100\n    audio_nbytes = 4\n    audio_bufsize = 2000\n    audio_codec = 'libmp3lame'\n    audio_extension = find_extension(audio_codec)\n    audiofile = f'./temp.{audio_extension}'\n\n    clip.audio.write_audiofile(audiofile,\n                              audio_fps,\n                              audio_nbytes,\n                              audio_bufsize,\n                              audio_codec\n                              )\n    ffmpeg_write_video(clip, filename, fps=clip.fps, audiofile=audiofile)\n    os.remove(audiofile)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:43:38.516589Z","iopub.execute_input":"2022-01-30T01:43:38.516968Z","iopub.status.idle":"2022-01-30T01:43:38.527363Z","shell.execute_reply.started":"2022-01-30T01:43:38.516927Z","shell.execute_reply":"2022-01-30T01:43:38.526025Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Print the duration of what should be a 10 second subclip.\nprint(clip.subclip(10, 20).duration)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:43:39.321242Z","iopub.execute_input":"2022-01-30T01:43:39.321657Z","iopub.status.idle":"2022-01-30T01:43:39.459274Z","shell.execute_reply.started":"2022-01-30T01:43:39.321620Z","shell.execute_reply":"2022-01-30T01:43:39.458214Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def create_subclip(clip, t_start, t_end):\n    newclip = clip.fl_time(lambda t: t + t_start, apply_to=['mask', 'audio'])\n    newclip.duration = t_end - t_start\n    newclip.end = newclip.start + newclip.duration\n    newclip.audio.duration = t_end - t_start\n    return newclip","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:43:41.430646Z","iopub.execute_input":"2022-01-30T01:43:41.431424Z","iopub.status.idle":"2022-01-30T01:43:41.436302Z","shell.execute_reply.started":"2022-01-30T01:43:41.431379Z","shell.execute_reply":"2022-01-30T01:43:41.435604Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Cut the original video to only the 30 seconds or so the Clinton is talking directly to the camera.\nnewclip = create_subclip(clip, 60, 92)\ncreate_videofile(newclip, './test.mp4')\nVideo('./test.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:43:44.587051Z","iopub.execute_input":"2022-01-30T01:43:44.587650Z","iopub.status.idle":"2022-01-30T01:43:54.990743Z","shell.execute_reply.started":"2022-01-30T01:43:44.587613Z","shell.execute_reply":"2022-01-30T01:43:54.989951Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":" [here]('https://github.com/ipazc/mtcnn').","metadata":{}},{"cell_type":"code","source":"!pip install mtcnn\nfrom mtcnn.mtcnn import MTCNN\n\nimg = newclip.get_frame(1)\nclf = MTCNN()\nbbox, *_ = clf.detect_faces(img)\nprint(bbox)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:04.193355Z","iopub.execute_input":"2022-01-30T01:44:04.193669Z","iopub.status.idle":"2022-01-30T01:44:21.662234Z","shell.execute_reply.started":"2022-01-30T01:44:04.193634Z","shell.execute_reply":"2022-01-30T01:44:21.660330Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"x, y, width, height = bbox['box']\nx2, y2 = x + width, y + height\nface_crop = img[y:y2, x:x2, :]\nplt.imshow(face_crop)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:21.664749Z","iopub.execute_input":"2022-01-30T01:44:21.665109Z","iopub.status.idle":"2022-01-30T01:44:21.885479Z","shell.execute_reply.started":"2022-01-30T01:44:21.665061Z","shell.execute_reply":"2022-01-30T01:44:21.884617Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def crop_face(gf, t):\n    frame = gf(t)\n    bbox, *_ = clf.detect_faces(frame)\n    x, y, x2, y2 = scale_bbox(bbox, frame.shape[:2], 2)\n    frame = frame[y:y2, x:x2, :]\n    frame = cv2.resize(frame, (48, 48))\n    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n    return frame\n\ndef scale_bbox(bbox, img_dims, scale=1.0):\n    ratio = np.sqrt(scale)\n    x, y, width, height = bbox['box']\n    cx, cy = x + width // 2, y + height // 2\n    new_w, new_h = width * np.sqrt(scale), height * np.sqrt(scale)\n    x, y = max(0, cx - (new_w // 2)), max(0, cy -  (new_h // 2))\n    x2, y2 = min(img_dims[1], x + new_w), min(img_dims[0], y + new_h)\n    return int(x), int(y), int(x2), int(y2)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:21.886818Z","iopub.execute_input":"2022-01-30T01:44:21.887159Z","iopub.status.idle":"2022-01-30T01:44:21.896170Z","shell.execute_reply.started":"2022-01-30T01:44:21.887119Z","shell.execute_reply":"2022-01-30T01:44:21.895269Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"frame = clip.get_frame(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:21.898775Z","iopub.execute_input":"2022-01-30T01:44:21.899055Z","iopub.status.idle":"2022-01-30T01:44:21.984314Z","shell.execute_reply.started":"2022-01-30T01:44:21.899019Z","shell.execute_reply":"2022-01-30T01:44:21.983281Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:21.986590Z","iopub.execute_input":"2022-01-30T01:44:21.986900Z","iopub.status.idle":"2022-01-30T01:44:21.991324Z","shell.execute_reply.started":"2022-01-30T01:44:21.986859Z","shell.execute_reply":"2022-01-30T01:44:21.990460Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"faceclip = newclip.fl(crop_face)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:21.993417Z","iopub.execute_input":"2022-01-30T01:44:21.994102Z","iopub.status.idle":"2022-01-30T01:44:22.637021Z","shell.execute_reply.started":"2022-01-30T01:44:21.994061Z","shell.execute_reply":"2022-01-30T01:44:22.636090Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(faceclip.get_frame((faceclip.duration // 5) * i), cmap='gray')\n    plt.title(f'Sample {i}')\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:22.638907Z","iopub.execute_input":"2022-01-30T01:44:22.639183Z","iopub.status.idle":"2022-01-30T01:44:25.781250Z","shell.execute_reply.started":"2022-01-30T01:44:22.639149Z","shell.execute_reply":"2022-01-30T01:44:25.780483Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"preds = np.zeros(shape=(int(faceclip.duration * faceclip.fps), 7))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:25.782755Z","iopub.execute_input":"2022-01-30T01:44:25.783566Z","iopub.status.idle":"2022-01-30T01:44:25.789364Z","shell.execute_reply.started":"2022-01-30T01:44:25.783505Z","shell.execute_reply":"2022-01-30T01:44:25.788686Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"import PIL\n#model.training = True\nfor i, frame in tqdm(enumerate(faceclip.iter_frames())):\n    img = test_transform(Image.fromarray(frame)).to(device)\n    with torch.no_grad():\n     #   assert model.training == False, 'model is in training mode'\n        outputs = model(img)\n    outputs = outputs.view(1, 10, -1)\n    outputs = (torch.sum(outputs, dim=1) / 10)\n    preds[i, :] = outputs.cpu()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:44:42.842256Z","iopub.execute_input":"2022-01-30T01:44:42.842525Z","iopub.status.idle":"2022-01-30T01:51:11.508275Z","shell.execute_reply.started":"2022-01-30T01:44:42.842496Z","shell.execute_reply":"2022-01-30T01:51:11.507451Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"preds_df = pd.DataFrame(preds, columns=emotions)\npreds_df['emotions'] = preds_df.idxmax(axis=1)\npreds_df['time'] = preds_df.index * 1/faceclip.fps\npreds_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.510450Z","iopub.execute_input":"2022-01-30T01:51:11.510993Z","iopub.status.idle":"2022-01-30T01:51:11.535235Z","shell.execute_reply.started":"2022-01-30T01:51:11.510950Z","shell.execute_reply":"2022-01-30T01:51:11.534519Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"preds_df = pd.DataFrame(preds, columns=emotions)\npreds_df['emotions'] = preds_df.idxmax(axis=1)\npreds_df['time'] = preds_df.index * 1/faceclip.fps\npreds_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.536690Z","iopub.execute_input":"2022-01-30T01:51:11.537005Z","iopub.status.idle":"2022-01-30T01:51:11.561122Z","shell.execute_reply.started":"2022-01-30T01:51:11.536968Z","shell.execute_reply":"2022-01-30T01:51:11.560366Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"preds_df['emotions'].hist()\nplt.title('Emotion Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.563534Z","iopub.execute_input":"2022-01-30T01:51:11.563841Z","iopub.status.idle":"2022-01-30T01:51:11.753164Z","shell.execute_reply.started":"2022-01-30T01:51:11.563803Z","shell.execute_reply":"2022-01-30T01:51:11.752441Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"for emotion in emotions:\n    preds_df[emotion] = (preds_df['emotions'] == emotion).rolling(window=25, min_periods=1).sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.754701Z","iopub.execute_input":"2022-01-30T01:51:11.755117Z","iopub.status.idle":"2022-01-30T01:51:11.766900Z","shell.execute_reply.started":"2022-01-30T01:51:11.755079Z","shell.execute_reply":"2022-01-30T01:51:11.766256Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"preds_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.768136Z","iopub.execute_input":"2022-01-30T01:51:11.768385Z","iopub.status.idle":"2022-01-30T01:51:11.786411Z","shell.execute_reply.started":"2022-01-30T01:51:11.768351Z","shell.execute_reply":"2022-01-30T01:51:11.785576Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"colourmap = {\n    'angry':'red',\n    'disgust':'pink',\n    'fear':'purple',\n    'happy':'green',\n    'sad':'blue',\n    'surprise':'yellow',\n    'neutral':'white'\n}","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.787573Z","iopub.execute_input":"2022-01-30T01:51:11.788381Z","iopub.status.idle":"2022-01-30T01:51:11.793330Z","shell.execute_reply.started":"2022-01-30T01:51:11.788309Z","shell.execute_reply":"2022-01-30T01:51:11.792460Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"preds_df[emotions] /= faceclip.fps","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.794806Z","iopub.execute_input":"2022-01-30T01:51:11.795112Z","iopub.status.idle":"2022-01-30T01:51:11.805751Z","shell.execute_reply.started":"2022-01-30T01:51:11.795078Z","shell.execute_reply":"2022-01-30T01:51:11.804979Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"preds_df","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.807003Z","iopub.execute_input":"2022-01-30T01:51:11.807364Z","iopub.status.idle":"2022-01-30T01:51:11.831170Z","shell.execute_reply.started":"2022-01-30T01:51:11.807324Z","shell.execute_reply":"2022-01-30T01:51:11.830298Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"preds_df = preds_df.set_index('time')\npreds_df[emotions].plot(figsize=(16, 8))\nplt.title('Proportion of Expression Predictions in the Previous Second')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:11.834920Z","iopub.execute_input":"2022-01-30T01:51:11.835110Z","iopub.status.idle":"2022-01-30T01:51:12.128133Z","shell.execute_reply.started":"2022-01-30T01:51:11.835086Z","shell.execute_reply":"2022-01-30T01:51:12.127394Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def annotate_frame(gf, t):\n    print(t)\n    frame = gf(t)\n    return frame\n\nfaceclip.fl(annotate_frame, apply_to=['mask'], keep_duration=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:12.129394Z","iopub.execute_input":"2022-01-30T01:51:12.129918Z","iopub.status.idle":"2022-01-30T01:51:12.724278Z","shell.execute_reply.started":"2022-01-30T01:51:12.129880Z","shell.execute_reply":"2022-01-30T01:51:12.723408Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"from matplotlib import colors\nfor key in colourmap.keys():\n    colourmap[key] = tuple(255 * x for x in colors.to_rgb(colourmap[key]))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:12.726183Z","iopub.execute_input":"2022-01-30T01:51:12.726462Z","iopub.status.idle":"2022-01-30T01:51:12.731092Z","shell.execute_reply.started":"2022-01-30T01:51:12.726420Z","shell.execute_reply":"2022-01-30T01:51:12.730218Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"colourmap","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:12.732530Z","iopub.execute_input":"2022-01-30T01:51:12.733067Z","iopub.status.idle":"2022-01-30T01:51:12.743338Z","shell.execute_reply.started":"2022-01-30T01:51:12.733029Z","shell.execute_reply":"2022-01-30T01:51:12.742668Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"frame = newclip.get_frame(0)\nbbox, *_ = clf.detect_faces(frame)\nx, y, x2, y2 = scale_bbox(bbox, frame.shape[:2], 2)\nemotion = preds_df['emotions'][0]\nrectangle(frame, (x, y), (x2, y2), colourmap[emotion], 1)\nfont = cv2.FONT_HERSHEY_SIMPLEX\n(width, height), baseline = cv2.getTextSize(f'{emotion}', font, 0.9, 1)\ncv2.putText(frame, f'{emotion}', (x, y+height), font, 0.9, colourmap[emotion], 1, \n            bottomLeftOrigin=False)\nplt.imshow(frame)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:12.744573Z","iopub.execute_input":"2022-01-30T01:51:12.744823Z","iopub.status.idle":"2022-01-30T01:51:13.446112Z","shell.execute_reply.started":"2022-01-30T01:51:12.744788Z","shell.execute_reply":"2022-01-30T01:51:13.445400Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def scale_bbox(bbox, img_dims, scale=1.0):\n    ratio = np.sqrt(scale)\n    x, y, width, height = bbox\n    cx, cy = x + width // 2, y + height // 2\n    new_w, new_h = width * np.sqrt(scale), height * np.sqrt(scale)\n    x, y = max(0, cx - (new_w // 2)), max(0, cy -  (new_h // 2))\n    x2, y2 = min(img_dims[1], x + new_w), min(img_dims[0], y + new_h)\n    return int(x), int(y), int(x2), int(y2)\n    \ndef process_frames(get_frame, t):\n    frame = get_frame(t)\n    bboxes = clf.detect_faces(frame)\n    bboxes = [x['box'] for x in bboxes]\n\n    for box in bboxes:\n\n        x, y, x2, y2 = scale_bbox(box, frame.shape[:2], 2)\n\n\n        face_crop = frame[y:y2, x:x2, :]\n        face_crop = cv2.resize(face_crop, (48, 48))\n        face_crop = cv2.cvtColor(face_crop, cv2.COLOR_RGB2GRAY)\n\n\n        outputs = model(test_transform(Image.fromarray(face_crop)).to(device))\n        outputs = outputs.view(1, 10, -1)\n        outputs = (torch.sum(outputs, dim=1) / 10)\n        emotion = emotions[outputs.argmax().item()]\n\n        frame = rectangle(frame, (x, y), (x2, y2), colourmap[emotion], 1)\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        (width, height), baseline = cv2.getTextSize(f'{emotion}', font, 0.9, 1)\n        frame = putText(frame, f'{emotion}', (x, y+height), font, 0.9, (255, 255, 255), 1, \n                    bottomLeftOrigin=False)\n        \n    return frame\n\ndef annotate_clip(video, save_path=None, t_start=0, t_end=None):\n\n    if validators.url(video):\n        try:\n            yt = YouTube(video)\n        except ValueError:\n            print('URL must be a youtube link.')\n        stream = yt.streams.filter(file_extension='mp4', progressive=True).get_highest_resolution()\n        stream.download('./', 'temp.mp4')\n        \n    clip = VideoFileClip('./temp.mp4')\n    \n    if t_end is None:\n        t_end = clip.end\n    clip = create_subclip(clip, t_start, t_end)\n    \n    clip = clip.fl(process_frames)\n\n    if save_path is not None:\n        create_videofile(clip, save_path)\n    os.remove('./temp.mp4')\n\n    return clip","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:13.447696Z","iopub.execute_input":"2022-01-30T01:51:13.448228Z","iopub.status.idle":"2022-01-30T01:51:13.464938Z","shell.execute_reply.started":"2022-01-30T01:51:13.448185Z","shell.execute_reply":"2022-01-30T01:51:13.464114Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nframe = newclip.get_frame(32)\nplt.subplot(1, 2, 1)\nplt.imshow(frame)\nplt.subplot(1, 2, 2)\nframe = process_frames(newclip.get_frame, 32)\nplt.imshow(frame)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:13.466216Z","iopub.execute_input":"2022-01-30T01:51:13.466607Z","iopub.status.idle":"2022-01-30T01:51:14.497204Z","shell.execute_reply.started":"2022-01-30T01:51:13.466518Z","shell.execute_reply":"2022-01-30T01:51:14.496438Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"!pip install validators\nimport validators","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:14.499996Z","iopub.execute_input":"2022-01-30T01:51:14.500559Z","iopub.status.idle":"2022-01-30T01:51:23.247569Z","shell.execute_reply.started":"2022-01-30T01:51:14.500495Z","shell.execute_reply":"2022-01-30T01:51:23.246729Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"clip = annotate_clip('https://www.youtube.com/watch?v=luLpdr4n8m4', './video_clips/t.mp4', t_start=60, t_end=92)\nVideo('./video_clips/t.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:51:23.248984Z","iopub.execute_input":"2022-01-30T01:51:23.249229Z","iopub.status.idle":"2022-01-30T01:58:04.021603Z","shell.execute_reply.started":"2022-01-30T01:51:23.249190Z","shell.execute_reply":"2022-01-30T01:58:04.020856Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"clip = annotate_clip('https://www.youtube.com/watch?v=NdAoQvqh7eY', './video_clips/keating.mp4')\nVideo('./video_clips/keating.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T02:19:14.205564Z","iopub.execute_input":"2022-01-30T02:19:14.206104Z","iopub.status.idle":"2022-01-30T02:25:57.128769Z","shell.execute_reply.started":"2022-01-30T02:19:14.206059Z","shell.execute_reply":"2022-01-30T02:25:57.128018Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"clip = annotate_clip('https://www.youtube.com/watch?v=ufhKWfPSQOw', './video_clips/yeonmipark.mp4', t_start=2, t_end=70)\nVideo('./video_clips/yeonmipark.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T02:25:57.130067Z","iopub.execute_input":"2022-01-30T02:25:57.130589Z","iopub.status.idle":"2022-01-30T02:47:02.691810Z","shell.execute_reply.started":"2022-01-30T02:25:57.130529Z","shell.execute_reply":"2022-01-30T02:47:02.691094Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"clip = annotate_clip('https://www.youtube.com/watch?v=dRQBtDtZTGA', './video_clips/davidmorrison.mp4', t_start=10, t_end=124)\nVideo('./video_clips/davidmorrison.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T02:47:02.693434Z","iopub.execute_input":"2022-01-30T02:47:02.693736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip = annotate_clip('https://www.youtube.com/watch?v=eKSgq-hxlaI', './video_clips/dlift.mp4', t_start=706, t_end=760)\nVideo('./video_clips/dlift.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:58:04.024415Z","iopub.execute_input":"2022-01-30T01:58:04.024762Z","iopub.status.idle":"2022-01-30T02:19:14.204242Z","shell.execute_reply.started":"2022-01-30T01:58:04.024729Z","shell.execute_reply":"2022-01-30T02:19:14.203374Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}